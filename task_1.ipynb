{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "сначала просто посмотрим как у нас уменьшается функция ошибок:",
   "id": "e97f1d6bb7de67ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T01:26:11.579419Z",
     "start_time": "2025-05-28T01:26:05.455909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, \"./Optimization-lib\")\n",
    "from lrs import *\n",
    "from gradient_descent import GradientDecent, ScipyWrapper\n",
    "from newton import NewtonCG\n",
    "from function_wrapper import FunctionWrapper\n",
    "from output import *\n",
    "from graphics_plotter import GraphicsPlotter\n",
    "from stochastic_gradient_descent import StochGradientDecent\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "wine_quality = fetch_ucirepo(id=186)\n",
    "\n",
    "X = wine_quality.data.features  # Признаки\n",
    "y = wine_quality.data.targets\n",
    "\n",
    "X = X.values\n",
    "y = y.values.ravel()\n",
    "\n",
    "def normalize(X_train):\n",
    "    # 1. Вычисляем среднее и стандартное отклонение ТОЛЬКО на тренировочных данных\n",
    "    mean = X_train.mean(axis=0)  # Среднее по каждому признаку\n",
    "    std = X_train.std(axis=0)     # Стандартное отклонение по каждому признаку\n",
    "    std[std == 0] = 1            # Избегаем деления на 0 (если std=0)\n",
    "\n",
    "    # 2. Нормализуем тренировочные и тестовые данные\n",
    "    X_train = (X_train - mean) / std\n",
    "    # X_test = (X_test - mean) / std\n",
    "\n",
    "    return X_train\n",
    "\n",
    "X = normalize(X)\n",
    "print(X)\n",
    "print(\"...\")\n",
    "print(y)\n",
    "\n",
    "def generate_weight_bounds(X, abs_bound: int) -> list:\n",
    "    \"\"\"\n",
    "    Генерирует границы весов для всех параметров модели (включая intercept)\n",
    "    n_features: Количество признаков (без учёта const)\n",
    "    \"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    return [[-1 * abs_bound, abs_bound] for _ in range(n_features + 1)]  # +1 для intercept\n",
    "\n",
    "def generate_start(X) -> list:\n",
    "    n_features = X.shape[1]\n",
    "    return [1 for _ in range(n_features + 1)]  # +1 для intercept\n",
    "\n",
    "\n",
    "bounds = generate_weight_bounds(X, 100)\n",
    "start = generate_start(X)\n",
    "\n",
    "\n",
    "gradient.clear()\n",
    "sgd = StochGradientDecent(exponential_decay(0.01, 0.0001), generate_weight_bounds(X, 1000), X, y, 5)\n",
    "error_min = sgd.find_min(generate_start(X), 100)\n",
    "print(error_min)\n",
    "\n",
    "pretty_dataset_print(sgd, \"EXP1\", error_min, gradient)\n",
    "\n",
    "\n",
    "/########################################################################\n",
    "\n",
    "\n",
    "# добавить график изменения значения error_min от количества итераций (?)\n",
    "#\n"
   ],
   "id": "acd377c786be9bd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14247327  2.18883292 -2.19283252 ...  1.81308951  0.19309677\n",
      "  -0.91546416]\n",
      " [ 0.45103572  3.28223494 -2.19283252 ... -0.11507303  0.99957862\n",
      "  -0.58006813]\n",
      " [ 0.45103572  2.55330026 -1.91755268 ...  0.25811972  0.79795816\n",
      "  -0.58006813]\n",
      " ...\n",
      " [-0.55179227 -0.6054167  -0.88525328 ... -1.42124765 -0.47897144\n",
      "  -0.91546416]\n",
      " [-1.32319841 -0.30169391 -0.12823371 ...  0.75571005 -1.016626\n",
      "   1.9354021 ]\n",
      " [-0.93749534 -0.78765037  0.42232597 ...  0.25811972 -1.41986693\n",
      "   1.09691202]]\n",
      "...\n",
      "[5 5 5 ... 6 7 6]\n",
      "1.1897035216105487\n",
      "\n",
      "        EXP1\n",
      "            found min error:         1.189704\n",
      "            found weights:      ['5.17575224846130854672', '0.23221107728766987588', '0.08473465432080465320', '0.05804017386406113560', '0.39784568165554545782', '0.12804677345021930890', '0.20685906818722618605', '0.11694056862290568011', '-0.06194505261198809953', '0.18325565384914738187', '-0.07865386711855928970', '0.75290825047093712374']\n",
      "            steps count:          100\n",
      "            function calls count: 4801\n",
      "            gradient calls count: 200\n",
      "            hessian calls count:  0\n",
      "          \n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "на каждом шаге мы ровно один раз считаем градиент (по формуле) и 1 раз функцию ошибок. Так что количество вызова функций = кол-ву шагов. Кажется можно выкинуть эти параметры из print_dataset_pretty.\n",
    "(Ps - можно считать градиент старым способом - будет сильно дольше (пару строк раскоментить и посмотреть сколько вызовов функции... -> сказать что мы молодцы и сразу градиет по формулу считаем. Так же с регулярностью - просто ее производную прибоаляем к градиету))"
   ],
   "id": "c38ae12c1acbf6b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "сейчас хотим выделять из dataset набор данных для обучения, и набор данных для проверки достоверности подобранных коэффициентов.\n",
    "\n"
   ],
   "id": "e62cb0c77bece5ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T01:26:15.606930Z",
     "start_time": "2025-05-28T01:26:11.592272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def custom_train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    \"\"\"просто рандомно делим на массивы\"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split_idx = int(len(X) * (1 - test_size))\n",
    "\n",
    "    X_train = X[indices[:split_idx]]\n",
    "    X_test = X[indices[split_idx:]]\n",
    "    y_train = y[indices[:split_idx]]\n",
    "    y_test = y[indices[split_idx:]]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = custom_train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "bounds = generate_weight_bounds(X, 100)\n",
    "start = generate_start(X)\n",
    "\n",
    "\n",
    "gradient.clear()\n",
    "sgd = StochGradientDecent(exponential_decay(0.01, 0.0001), generate_weight_bounds(X_train, 1000), X_train, y_train, 5)\n",
    "error_min = sgd.find_min(generate_start(X_train), 120) # !! полезно менять max_steps\n",
    "pretty_dataset_print(sgd, \"EXP1\", error_min, gradient)\n",
    "\n",
    "# Получаем обученные веса\n",
    "trained_weights = sgd.get_x()\n",
    "\n",
    "# Прогнозируем на тестовой выборке\n",
    "X_test = np.c_[np.ones(X_test.shape[0]), X_test] # это мы добавили столбец 1 что бы отдельно с константами не возиться (1, x1, x2...)\n",
    "y_pred = np.dot(X_test, trained_weights)  # предсказание результата\n",
    "\n",
    "\n",
    "# Вычисляем метрики качества на тестовой выборке\n",
    "# сюда вскяие функции для анализа впихнуть и вывод написать что мы молодцы\n",
    "\n",
    "print(\"\\nРезультаты на тестовой выборке:\")\n",
    "print(\"True values:\", y_test)\n",
    "print(\"Predicted values:\", y_pred)\n",
    "#\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "def mean_absolute_error(y_true, y_pred):\n",
    "    return np.abs(y_true - y_pred).mean()\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nMSE: {mse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "# print(f\"R2 Score: {r2:.4f}\")\n",
    "\n"
   ],
   "id": "c2cbc3a70e728a45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        EXP1\n",
      "            found min error:         1.140617\n",
      "            found weights:      ['5.28782849775108854118', '0.25866594877269261099', '0.14873552789412053388', '0.15811103632418255227', '0.22299977638661713786', '-0.04987182617298462767', '0.14806890090945951099', '0.24801519411139943960', '-0.01072839543144898933', '0.37947050117061664753', '0.11656715171751827476', '0.72660682683487420874']\n",
      "            steps count:          120\n",
      "            function calls count: 5761\n",
      "            gradient calls count: 240\n",
      "            hessian calls count:  0\n",
      "          \n",
      "\n",
      "Результаты на тестовой выборке:\n",
      "True values: [7 7 5 ... 5 5 5]\n",
      "Predicted values: [6.19866864 6.63806819 5.75763635 ... 4.85995832 5.1101066  4.96203293]\n",
      "\n",
      "MSE: 1.1602\n",
      "MAE: 0.8363\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
