{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": "from ucimlrepo import fetch_ucirepo, list_available_datasets",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ucimlrepo import fetch_ucirepo, list_available_datasets\n",
    "\n",
    "# check which datasets can be imported\n",
    "list_available_datasets()\n",
    "\n",
    "# import dataset\n",
    "heart_disease = fetch_ucirepo(id=45)\n",
    "# alternatively: fetch_ucirepo(name='Heart Disease')\n",
    "\n",
    "# access data\n",
    "X = heart_disease.data.features\n",
    "y = heart_disease.data.targets\n",
    "# train model e.g. sklearn.linear_model.LinearRegression().fit(X, y)\n",
    "\n",
    "# access metadata\n",
    "print(heart_disease.metadata.uci_id)\n",
    "print(heart_disease.metadata.num_instances)\n",
    "print(heart_disease.metadata.additional_info.summary)\n",
    "\n",
    "# access variable info in tabular format\n",
    "print(heart_disease.variables)"
   ],
   "id": "e3ffedf1b7caeaff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "wine_quality = fetch_ucirepo(id=186)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X = wine_quality.data.features\n",
    "y = wine_quality.data.targets\n",
    "\n",
    "# metadata\n",
    "print(wine_quality.metadata)\n",
    "\n",
    "# variable information\n",
    "print(wine_quality.variables)\n",
    "\n",
    "\n",
    "print(wine_quality.variable)\n",
    "info = wine_quality.metadata.additional_info\n",
    "for i in info:\n",
    "    print(i+\"::\" , info.get(i))"
   ],
   "id": "21cfcab5bfcbfecb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T14:41:11.374129Z",
     "start_time": "2025-05-27T14:41:06.288115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"./Optimization-lib\")\n",
    "from lrs import *\n",
    "from gradient_descent import GradientDecent, ScipyWrapper\n",
    "from newton import NewtonCG\n",
    "from function_wrapper import FunctionWrapper\n",
    "from output import pretty_print\n",
    "from graphics_plotter import GraphicsPlotter\n",
    "\n",
    "\n",
    "func = FunctionWrapper(lambda x: x[0] ** 2 + 100 * x[1]**2)\n",
    "\n",
    "func.clear()\n",
    "gradient.clear()\n",
    "hessian.clear()\n",
    "linear_search = GradientDecent(exponential_decay(0.01, 0.001), func, [[-6.0, 6.0], [-6.0, 6.0]], 0.0001, gradient)\n",
    "test_painter_1 = GraphicsPlotter(linear_search)\n",
    "res_1 = linear_search.find_min([5, 3], 100000)\n",
    "pretty_print(linear_search, \"EXP\", res_1, gradient)\n",
    "\n",
    "\n",
    "func.clear()\n",
    "gradient.clear()\n",
    "hessian.clear()\n",
    "linear_search_stoch = GradientDecent(exponential_decay(0.01, 0.001), func, [[-6.0, 6.0], [-6.0, 6.0]], 0.000001, gradient_stochastic)\n",
    "test_painter_1 = GraphicsPlotter(linear_search_stoch)\n",
    "res_1 = linear_search_stoch.find_min([5, 3], 100000)\n",
    "pretty_print(linear_search_stoch, \"EXP\", res_1, gradient_stochastic)\n",
    "\n",
    "# func = lambda: FunctionWrapper(lambda x: (x[0] - 2)**2 + (x[1] - 4)**2 + 1)\n",
    "# bounds = [[-10, 15], [-10, 15]]\n",
    "#\n",
    "# grad_dec = GradientDecent(constant(2), func, bounds, 0.0001, gradient)\n",
    "# my_min = grad_dec.find_min([2, 3], 1000)\n",
    "# pretty_print(grad_dec, \"const\", my_min, gradient)\n",
    "\n",
    "\n"
   ],
   "id": "a77fb4b328ddd49f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        EXP\n",
      "            found result:         0.000053\n",
      "            found result in:      ['0.00729093304433353108', '0.00000000000000000001']\n",
      "            steps count:          392\n",
      "            function calls count: 1569\n",
      "            gradient calls count: 392\n",
      "            hessian calls count:  0\n",
      "          \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FunctionWrapper' object has no attribute 'dot'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 27\u001B[39m\n\u001B[32m     25\u001B[39m linear_search_stoch = GradientDecent(exponential_decay(\u001B[32m0.01\u001B[39m, \u001B[32m0.001\u001B[39m), func, [[-\u001B[32m6.0\u001B[39m, \u001B[32m6.0\u001B[39m], [-\u001B[32m6.0\u001B[39m, \u001B[32m6.0\u001B[39m]], \u001B[32m0.000001\u001B[39m, gradient_stochastic)\n\u001B[32m     26\u001B[39m test_painter_1 = GraphicsPlotter(linear_search_stoch)\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m res_1 = \u001B[43mlinear_search_stoch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfind_min\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m100000\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m     28\u001B[39m pretty_print(linear_search_stoch, \u001B[33m\"\u001B[39m\u001B[33mEXP\u001B[39m\u001B[33m\"\u001B[39m, res_1, gradient_stochastic)\n\u001B[32m     30\u001B[39m \u001B[38;5;66;03m# func = lambda: FunctionWrapper(lambda x: (x[0] - 2)**2 + (x[1] - 4)**2 + 1)\u001B[39;00m\n\u001B[32m     31\u001B[39m \u001B[38;5;66;03m# bounds = [[-10, 15], [-10, 15]]\u001B[39;00m\n\u001B[32m     32\u001B[39m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[32m     33\u001B[39m \u001B[38;5;66;03m# grad_dec = GradientDecent(constant(2), func, bounds, 0.0001, gradient)\u001B[39;00m\n\u001B[32m     34\u001B[39m \u001B[38;5;66;03m# my_min = grad_dec.find_min([2, 3], 1000)\u001B[39;00m\n\u001B[32m     35\u001B[39m \u001B[38;5;66;03m# pretty_print(grad_dec, \"const\", my_min, gradient)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Progets_Metopts\\Optimization-Methods-lab3\\Optimization-lib\\gradient_descent.py:59\u001B[39m, in \u001B[36mGradientDecent.find_min\u001B[39m\u001B[34m(self, start, max_iterations)\u001B[39m\n\u001B[32m     52\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfind_min\u001B[39m(\u001B[38;5;28mself\u001B[39m, start: List[\u001B[38;5;28mfloat\u001B[39m], max_iterations: \u001B[38;5;28mint\u001B[39m) -> \u001B[38;5;28mfloat\u001B[39m:\n\u001B[32m     53\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     54\u001B[39m \u001B[33;03m        :param  start: List[float] - стартовая точка, в которой начнём поиск\u001B[39;00m\n\u001B[32m     55\u001B[39m \u001B[33;03m        :param  max_iterations: int - максимальное количество итераций спуска\u001B[39;00m\n\u001B[32m     56\u001B[39m \u001B[33;03m        :return: - минимум полученный в ходе спуска\u001B[39;00m\n\u001B[32m     57\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m59\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m__find\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstart\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_iterations\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Progets_Metopts\\Optimization-Methods-lab3\\Optimization-lib\\gradient_descent.py:40\u001B[39m, in \u001B[36mGradientDecent.__find\u001B[39m\u001B[34m(self, start, max_iterations, op)\u001B[39m\n\u001B[32m     38\u001B[39m h = \u001B[38;5;28mself\u001B[39m.learning_rate_scheduling(\u001B[38;5;28mself\u001B[39m.x, i, \u001B[38;5;28mself\u001B[39m.f, \u001B[38;5;28mself\u001B[39m.bounds)\n\u001B[32m     39\u001B[39m \u001B[38;5;28mself\u001B[39m.path.append(\u001B[38;5;28mself\u001B[39m.x)\n\u001B[32m---> \u001B[39m\u001B[32m40\u001B[39m grad = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43meps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     41\u001B[39m xx = []\n\u001B[32m     42\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m.x)):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Progets_Metopts\\Optimization-Methods-lab3\\Optimization-lib\\function_wrapper.py:35\u001B[39m, in \u001B[36mFunctionWrapper.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m     32\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._cache[key]\n\u001B[32m     34\u001B[39m \u001B[38;5;28mself\u001B[39m.count += \u001B[32m1\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m35\u001B[39m result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     36\u001B[39m \u001B[38;5;28mself\u001B[39m._cache[key] = result\n\u001B[32m     37\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\Progets_Metopts\\Optimization-Methods-lab3\\Optimization-lib\\lrs.py:31\u001B[39m, in \u001B[36m__gradient_stochastic\u001B[39m\u001B[34m(X_batch, y_batch, x)\u001B[39m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__gradient_stochastic\u001B[39m(X_batch: np.array, y_batch: np.array, x: np.array) -> np.array:\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m     error = y_batch - \u001B[43mX_batch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdot\u001B[49m(x)\n\u001B[32m     32\u001B[39m     grad = \u001B[32m2\u001B[39m * X_batch.T.dot(error) / \u001B[38;5;28mlen\u001B[39m(X_batch)\n\u001B[32m     33\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m grad\n",
      "\u001B[31mAttributeError\u001B[39m: 'FunctionWrapper' object has no attribute 'dot'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# num_instances: Количество строк или выборок\n",
    "# num_features: Количество столбцов характеристик\n",
    "# feature_types: Типы данных признаков\n",
    "# target_col: Имя целевого столбца (столбцов)\n",
    "# index_col: Имя столбца (столбцов) индекса\n",
    "# has_missing_values: содержит ли набор данных отсутствующие значения\n",
    "# missing_values_symbol: Указывает, какой символ представляет отсутствующие записи (если в наборе данных есть отсутствующие значения)\n",
    "#\n",
    "\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "wine_quality = fetch_ucirepo(id=186)\n",
    "\n",
    "metadata = wine_quality.metadata\n",
    "print(\"Информация о наборе данных Wine Quality:\")\n",
    "print(\"----------------------------------------\")\n",
    "print(f\"num_instances: {metadata.num_instances} (Количество строк или выборок)\")\n",
    "print(f\"num_features: {metadata.num_features} (Количество столбцов характеристик)\")\n",
    "print(f\"feature_types: {metadata.feature_types} (Типы данных признаков)\")\n",
    "print(f\"target_col: {metadata.target_col} (Имя целевого столбца (столбцов))\")\n",
    "print(f\"index_col: {metadata.index_col} (Имя столбца (столбцов) индекса)\")\n",
    "print(f\"has_missing_values: {metadata.has_missing_values} (Содержит ли набор данных отсутствующие значения)\")\n",
    "print(f\"missing_values_symbol: {metadata.missing_values_symbol} (Символ, представляющий отсутствующие записи)\")\n",
    "print(\"----------------------------------------\")\n",
    "func = FunctionWrapper(lambda x: x[0] ** 2 + 100 * x[1])"
   ],
   "id": "c40b059bf625c1c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T15:07:59.019645Z",
     "start_time": "2025-05-27T15:07:52.370007Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"./Optimization-lib\")\n",
    "from lrs import *\n",
    "from gradient_descent import GradientDecent, ScipyWrapper\n",
    "from newton import NewtonCG\n",
    "from function_wrapper import FunctionWrapper\n",
    "from output import pretty_print\n",
    "from graphics_plotter import GraphicsPlotter\n",
    "from stochastic_gradient_descent import StochGradientDecent\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "wine_quality = fetch_ucirepo(id=186)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = wine_quality.data.features  # Признаки\n",
    "y = wine_quality.data.targets\n",
    "\n",
    "X = X.values\n",
    "y = y.values.ravel()\n",
    "\n",
    "print(X)\n",
    "print(\"...\")\n",
    "print(y)\n",
    "\n",
    "def generate_weight_bounds(X, abs_bound: int) -> list:\n",
    "    \"\"\"\n",
    "    Генерирует границы весов для всех параметров модели (включая intercept)\n",
    "    n_features: Количество признаков (без учёта const)\n",
    "    \"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    return [[-1 * abs_bound, abs_bound] for _ in range(n_features + 1)]  # +1 для intercept\n",
    "\n",
    "def generate_start(X) -> list:\n",
    "    n_features = X.shape[1]\n",
    "    return [1 for _ in range(n_features + 1)]  # +1 для intercept\n",
    "\n",
    "\n",
    "bounds = generate_weight_bounds(X, 100)\n",
    "start = generate_start(X)\n",
    "\n",
    "\n",
    "gradient.clear()\n",
    "sgd = StochGradientDecent(exponential_decay(0.01, 0.001), generate_weight_bounds(X, 1000), X, y, 5)\n",
    "error_min = sgd.find_min(generate_start(X), 10)\n",
    "print(error_min)\n",
    "\n",
    "pretty_print(sgd, \"EXP\", error_min, gradient)\n",
    "#\n",
    "#\n",
    "# predictions = np.dot(X_batch, weights) + bias\n",
    "#\n",
    "# def compute_mse(y_true, y_pred):\n",
    "#     return np.mean((y_true - y_pred)**2)\n",
    "#\n",
    "#\n",
    "# X_batch = X_train_shuffled[i:i+batch_size]\n",
    "#\n"
   ],
   "id": "cd05f403f6ae5f79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.4   0.7   0.   ...  3.51  0.56  9.4 ]\n",
      " [ 7.8   0.88  0.   ...  3.2   0.68  9.8 ]\n",
      " [ 7.8   0.76  0.04 ...  3.26  0.65  9.8 ]\n",
      " ...\n",
      " [ 6.5   0.24  0.19 ...  2.99  0.46  9.4 ]\n",
      " [ 5.5   0.29  0.3  ...  3.34  0.38 12.8 ]\n",
      " [ 6.    0.21  0.38 ...  3.26  0.32 11.8 ]]\n",
      "...\n",
      "[5 5 5 ... 6 7 6]\n",
      "4.33922832\n",
      "25.30456008\n",
      "2.4357297944\n",
      "1.9068305967999999\n",
      "12.502252064\n",
      "1.26296746328\n",
      "115.59524768\n",
      "453.65959167999995\n",
      "4.320806911391999\n",
      "11.804848064800002\n",
      "3.0407496256\n",
      "34.966498103999996\n",
      "[  4.33922832  25.30456008   2.43572979   1.9068306   12.50225206\n",
      "   1.26296746 115.59524768 453.65959168   4.32080691  11.80484806\n",
      "   3.04074963  34.9664981 ]\n",
      "1049.1814472068793\n",
      "7665.13376692476\n",
      "273.20062709122215\n",
      "321.2561846283158\n",
      "8995.202928783016\n",
      "50.00677275286289\n",
      "46182.23702276782\n",
      "161998.91205084213\n",
      "1045.9065542370988\n",
      "3421.361403236282\n",
      "623.4119025449204\n",
      "10358.270527182252\n",
      "[1.04918145e+03 7.66513377e+03 2.73200627e+02 3.21256185e+02\n",
      " 8.99520293e+03 5.00067728e+01 4.61822370e+04 1.61998912e+05\n",
      " 1.04590655e+03 3.42136140e+03 6.23411903e+02 1.03582705e+04]\n",
      "422736.4336980863\n",
      "2885782.14055089\n",
      "187139.13414233807\n",
      "201816.9977270754\n",
      "3931229.0353589654\n",
      "19615.805285337156\n",
      "16809618.40439382\n",
      "67071531.21535607\n",
      "420835.84504995774\n",
      "1313719.8401642912\n",
      "250124.07503757896\n",
      "4294732.614059344\n",
      "[4.22736434e+05 2.88578214e+06 1.87139134e+05 2.01816998e+05\n",
      " 3.93122904e+06 1.96158053e+04 1.68096184e+07 6.70715312e+07\n",
      " 4.20835845e+05 1.31371984e+06 2.50124075e+05 4.29473261e+06]\n",
      "194473158.65478268\n",
      "1379848435.885626\n",
      "65200115.1292946\n",
      "70250465.28114246\n",
      "2042256405.4437797\n",
      "9198335.695830245\n",
      "9344273182.677792\n",
      "36476750504.39955\n",
      "193905895.55802613\n",
      "604398494.4578947\n",
      "98611827.70860176\n",
      "1892519145.4920967\n",
      "[1.94473159e+08 1.00000000e+09 6.52001151e+07 7.02504653e+07\n",
      " 1.00000000e+09 9.19833570e+06 1.00000000e+09 1.00000000e+09\n",
      " 1.93905896e+08 6.04398494e+08 9.86118277e+07 1.00000000e+09]\n",
      "3337815221.475583\n",
      "23576919720.626606\n",
      "967019801.7938559\n",
      "1008373510.8680799\n",
      "19109039446.78373\n",
      "146641594.22275007\n",
      "95083893586.75543\n",
      "380509620419.2935\n",
      "3317513219.4538317\n",
      "10323033891.239954\n",
      "1750360892.1695883\n",
      "34330137399.941082\n",
      "[1.00000000e+09 1.00000000e+09 9.67019802e+08 1.00000000e+09\n",
      " 1.00000000e+09 1.46641594e+08 1.00000000e+09 1.00000000e+09\n",
      " 1.00000000e+09 1.00000000e+09 1.00000000e+09 1.00000000e+09]\n",
      "4345163498.183718\n",
      "27978776584.145924\n",
      "2046480139.1459405\n",
      "2358174957.4530015\n",
      "27891759782.67359\n",
      "309550690.6811655\n",
      "140330938002.64087\n",
      "447463862999.75885\n",
      "4331945628.454554\n",
      "11309224134.445614\n",
      "2725935769.7590246\n",
      "35086762947.162506\n",
      "[1.00000000e+09 1.00000000e+09 1.00000000e+09 1.00000000e+09\n",
      " 1.00000000e+09 3.09550691e+08 1.00000000e+09 1.00000000e+09\n",
      " 1.00000000e+09 1.00000000e+09 1.00000000e+09 1.00000000e+09]\n",
      "3637517839.4290295\n",
      "17973523173.586758\n",
      "1767303534.798469\n",
      "1740761231.4691107\n",
      "19231988704.603195\n",
      "447224866.6569699\n",
      "96452074827.66423\n",
      "274130852705.50882\n",
      "3623124147.514486\n",
      "9707196586.141994\n",
      "2285947249.805427\n",
      "28901121139.231197\n",
      "[1.00000000e+09 1.00000000e+09 1.00000000e+09 1.00000000e+09\n",
      " 1.00000000e+09 4.47224867e+08 1.00000000e+09 1.00000000e+09\n",
      " 1.00000000e+09 1.00000000e+09 1.00000000e+09 1.00000000e+09]\n",
      "4104956608.806877\n",
      "23594543236.794327\n",
      "1824433846.066353\n",
      "1975732607.3537104\n",
      "5806597649.270274\n",
      "608273071.8488054\n",
      "118910685900.36641\n",
      "457121043756.2473\n",
      "4084555349.564307\n",
      "11013798934.211565\n",
      "2470329396.9138365\n",
      "32476640469.752922\n",
      "[1.00000000e+09 1.00000000e+09 1.00000000e+09 1.00000000e+09\n",
      " 1.00000000e+09 6.08273072e+08 1.00000000e+09 1.00000000e+09\n",
      " 1.00000000e+09 1.00000000e+09 1.00000000e+09 1.00000000e+09]\n",
      "4526738468.738865\n",
      "25753660597.59479\n",
      "2303334826.8752766\n",
      "2041518909.1993585\n",
      "34944057094.48073\n",
      "771755466.1712661\n",
      "103428933170.87305\n",
      "493635988382.137\n",
      "4510916718.324333\n",
      "12350719951.287365\n",
      "2765131715.9685717\n",
      "37596197262.20375\n",
      "[1.00000000e+09 1.00000000e+09 1.00000000e+09 1.00000000e+09\n",
      " 1.00000000e+09 7.71755466e+08 1.00000000e+09 1.00000000e+09\n",
      " 1.00000000e+09 1.00000000e+09 1.00000000e+09 1.00000000e+09]\n",
      "4328128681.469643\n",
      "24947748270.67824\n",
      "2003350223.4989502\n",
      "2485421953.1263876\n",
      "18974680721.053967\n",
      "950307132.3416729\n",
      "106124355807.15523\n",
      "393005154889.8386\n",
      "4311983170.940733\n",
      "11955329512.619698\n",
      "2853929180.14298\n",
      "35496100531.82677\n",
      "[1.00000000e+09 1.00000000e+09 1.00000000e+09 1.00000000e+09\n",
      " 1.00000000e+09 9.50307132e+08 1.00000000e+09 1.00000000e+09\n",
      " 1.00000000e+09 1.00000000e+09 1.00000000e+09 1.00000000e+09]\n",
      "3.611939053201846e+22\n",
      "\n",
      "        EXP\n",
      "            found result:         36119390532018460688384.000000\n",
      "            found result in:      ['1000000000.00000000000000000000', '1000000000.00000000000000000000', '1000000000.00000000000000000000', '1000000000.00000000000000000000', '1000000000.00000000000000000000', '771755466.17126607894897460938', '1000000000.00000000000000000000', '1000000000.00000000000000000000', '1000000000.00000000000000000000', '1000000000.00000000000000000000', '1000000000.00000000000000000000', '1000000000.00000000000000000000']\n",
      "            steps count:          10\n",
      "            function calls count: 1\n",
      "            gradient calls count: 0\n",
      "            hessian calls count:  0\n",
      "          \n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
